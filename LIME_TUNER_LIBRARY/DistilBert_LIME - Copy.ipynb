{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "import scipy\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tuner_library\n",
    "import utilities as util\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "#dictionary [key, explanation]\n",
    "\n",
    "class tuner:\n",
    "    def __init__(self, text, filename, class_names):\n",
    "        self.dict = {}\n",
    "        self.sigmas = []\n",
    "        self.text = text\n",
    "        self.filename = filename\n",
    "        self.class_names = class_names\n",
    "    \n",
    "    def initialize():\n",
    "        self.dict.clear()\n",
    "        return\n",
    "    \n",
    "    #get the blackbox prediction for a text instance using filename(model.pkl file) \n",
    "    def get_Prediction(self):\n",
    "        print(\"HII\")\n",
    "        loaded_model = pickle.load(open(self.filename, 'rb'))\n",
    "        result = loaded_model.predict_proba([self.text])\n",
    "        return json.dumps(result.tolist())\n",
    "    \n",
    "    #get the explanations or fill up all the explanantion field for a particular sigma in dictionary with key value as sigma\n",
    "    def get_Explanation(self, sigma):\n",
    "        if sigma in self.dict:\n",
    "            print(\"YES\")\n",
    "            return self.dict[sigma]\n",
    "        else:\n",
    "            explainer = LimeTextExplainer(kernel_width=sigma,class_names=self.class_names)\n",
    "            print(\"sigma = \" + str(sigma))\n",
    "            black_box = pickle.load(open(self.filename, 'rb'))\n",
    "            exp = explainer.explain_instance(self.text, black_box.predict_proba, num_features=6)\n",
    "            self.dict[sigma] = exp\n",
    "            return self.dict[sigma]\n",
    "        return \n",
    "    \n",
    "    #to plot the entropy v/s sigma curve, this fuction calulates the value of entropy for all the sigmas and then return sigmas, entropies\n",
    "    def get_Sigma_Entropy(self):\n",
    "        sigmas = []\n",
    "        entropies = []\n",
    "        for key in sorted (self.dict.keys()):\n",
    "            weight_per_bucket = []\n",
    "            for b in range(11):\n",
    "                weight_per_bucket.append(0)\n",
    "            exp = self.dict[key]\n",
    "            list_pair = [(p1, p2) for idx1, p1 in enumerate(exp.weights) for idx2, p2 in enumerate(exp.bb_labels) if idx1==idx2]\n",
    "            list_pair.sort()\n",
    "            for t in list_pair:\n",
    "                if(t[1] >= 0 and t[1] < 0.1):\n",
    "                    weight_per_bucket[0] += t[0]\n",
    "                elif(t[1] >= 0.1 and t[1] < 0.2):\n",
    "                    weight_per_bucket[1] += t[0]\n",
    "                elif(t[1] >= 0.2 and t[1] < 0.3):\n",
    "                    weight_per_bucket[2] += t[0]\n",
    "                elif(t[1] >= 0.3 and t[1] < 0.4):\n",
    "                    weight_per_bucket[3] += t[0]\n",
    "                elif(t[1] >= 0.4 and t[1] < 0.5):\n",
    "                    weight_per_bucket[4] += t[0]\n",
    "                elif(t[1] >= 0.5 and t[1] < 0.6):\n",
    "                    weight_per_bucket[5] += t[0]\n",
    "                elif(t[1] >= 0.6 and t[1] < 0.7):\n",
    "                    weight_per_bucket[6] += t[0]\n",
    "                elif(t[1] >= 0.7 and t[1] < 0.8):\n",
    "                    weight_per_bucket[7] += t[0]\n",
    "                elif(t[1] >= 0.8 and t[1] < 0.9):\n",
    "                    weight_per_bucket[8] += t[0]\n",
    "                elif(t[1] >= 0.9 and t[1] < 1.0):\n",
    "                    weight_per_bucket[9] += t[0]\n",
    "                else :\n",
    "                    weight_per_bucket[10] += t[0]\n",
    "            norm_weight_per_bucket = [float(i)/sum(weight_per_bucket) for i in weight_per_bucket]\n",
    "            entropy = scipy.stats.entropy(norm_weight_per_bucket)\n",
    "            self.dict[key].entropy = entropy\n",
    "            sigmas.append(key)\n",
    "            entropies.append(entropy)\n",
    "        return sigmas, entropies\n",
    "      \n",
    "    #to be called only after the get_Explanation for sigma is called\n",
    "    #used to plot the weight distribution plot of neighbourhood points (vary for each sigma)\n",
    "    def get_Weight_Distribution_Plot_Per_Sigma(self, sigma):\n",
    "        return self.dict[sigma].weights, self.dict[sigma].bb_labels\n",
    "    \n",
    "    #to be called only after the get_Explanation for sigma is called\n",
    "    #returns all the 4 models' rmse (constant model, linear model(used by LIME), decision tree, random forests)    \n",
    "    def get_RMSEs_And_Sigmas(self):\n",
    "        constant_rmse = []\n",
    "        linear_rmse = []\n",
    "        decisionTree_rmse = []\n",
    "        randomForest_rmse = []\n",
    "        sigmas = []\n",
    "        for key in sorted (self.dict.keys()):\n",
    "            sigmas.append(key)\n",
    "            constant_rmse.append(self.dict[key].naive_score)\n",
    "            linear_rmse.append(self.dict[key].score)\n",
    "            decisionTree_rmse.append(self.dict[key].rf_score)\n",
    "            randomForest_rmse.append(self.dict[key].rf_2_score)\n",
    "        return sigmas, constant_rmse, linear_rmse, decisionTree_rmse, randomForest_rmse\n",
    "    \n",
    "    def getRange_Constant_Model(self):\n",
    "        sigmas,constant_rmse, linear_rmse, decisionTree_rmse, randomForest_rmse = self.get_RMSEs_And_Sigmas()\n",
    "        const_range = []\n",
    "        const_range.append(0.39)\n",
    "        for i in range(len(sigmas)):\n",
    "            if(linear_rmse[i] < constant_rmse[i]):\n",
    "                const_range.append(sigmas[i])\n",
    "                return const_range\n",
    "        const_range.append(0.39)\n",
    "        return const_range\n",
    "    \n",
    "    def getRange_Linear_Model(self):\n",
    "        sigmas,constant_rmse, linear_rmse, decisionTree_rmse, randomForest_rmse = self.get_RMSEs_And_Sigmas()\n",
    "        const_range = self.getRange_Constant_Model()\n",
    "        linear_range = []\n",
    "        linear_range.append(const_range[1])\n",
    "        diff = [m - n for m,n in zip(linear_rmse,randomForest_rmse)]\n",
    "        for i in range(len(sigmas)):\n",
    "            if(diff[len(sigmas)-i-1] < 0):\n",
    "                linear_range.append(sigmas[len(sigmas)-i-1])\n",
    "                return linear_range\n",
    "        linear_range.append(const_range[1])\n",
    "        return linear_range\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['negative',  'positive']\n",
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 = df[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## Want BERT instead of distilBERT? Uncomment the following line:\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 59)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 59)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_ids = input_ids.type(torch.LongTensor)\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "labels = batch_1[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class Bert_LR(LogisticRegression):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = pd.Series(X)\n",
    "        tokenized = X.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "        max_len = 0\n",
    "        for i in tokenized.values:\n",
    "            if len(i) > max_len:\n",
    "                max_len = len(i)\n",
    "\n",
    "        padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        input_ids = torch.tensor(padded)  \n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_ids = input_ids.type(torch.LongTensor)\n",
    "            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        features = last_hidden_states[0][:,0,:].numpy() \n",
    "        scores = self.decision_function(features)\n",
    "        if len(scores.shape) == 1:\n",
    "            indices = (scores > 0).astype(np.int)\n",
    "        else:\n",
    "            indices = scores.argmax(axis=1)\n",
    "        return self.classes_[indices]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        ovr = (self.multi_class in [\"ovr\", \"warn\"] or\n",
    "               (self.multi_class == 'auto' and (self.classes_.size <= 2 or\n",
    "                                                self.solver == 'liblinear')))\n",
    "        X = pd.Series(X)\n",
    "        tokenized = X.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "        max_len = 0\n",
    "        for i in tokenized.values:\n",
    "            if len(i) > max_len:\n",
    "                max_len = len(i)\n",
    "\n",
    "        padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        input_ids = torch.tensor(padded)  \n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_ids = input_ids.type(torch.LongTensor)\n",
    "            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        features = last_hidden_states[0][:,0,:].numpy()\n",
    "\n",
    "        if ovr:\n",
    "            return super()._predict_proba_lr(features)\n",
    "        else:\n",
    "            decision = self.decision_function(features)\n",
    "            if decision.ndim == 1:\n",
    "                # Workaround for multi_class=\"multinomial\" and binary outcomes\n",
    "                # which requires softmax prediction with only a 1D decision.\n",
    "                decision_2d = np.c_[-decision, decision]\n",
    "            else:\n",
    "                decision_2d = decision\n",
    "            return softmax(decision_2d, copy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = Bert_LR(tokenizer)\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf.predict_proba(batch_1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_2 = df[2000:]\n",
    "y_pred_class = lr_clf.predict(batch_2[0])\n",
    "print(metrics.accuracy_score(batch_2[1], y_pred_class))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lr_clf, open(\"distilBert_model.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = batch_1[0][2]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tuner_library import tuner\n",
    "tl = tuner(text,'distilBert_model.pkl', class_names)\n",
    "result = util.JsonToArray(tl.get_Prediction()) \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 25\n",
    "#get the explanation for a particular sigma (all the explanation fields are filled)\n",
    "exp = tl.get_Explanation(sigma)\n",
    "print(exp.rf_score)\n",
    "exp.save_to_file('explanation_25.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0.39062499968\n",
    "tl.sigmas = k*2**np.arange(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sigma in tl.sigmas:\n",
    "    exp = tl.get_Explanation(sigma) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for plotting entropy v/s sigma curve\n",
    "sigmas, entropies = tl.get_Sigma_Entropy()\n",
    "#get the rmse values(all the explanation models) for all the sigmas \n",
    "sigmas,constant_rmse, linear_rmse, decisionTree_rmse, randomForest_rmse = tl.get_RMSEs_And_Sigmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the sigma v/s rmses \n",
    "axes=plt.gca()\n",
    "\n",
    "plt.xscale('log', basex = 2)\n",
    "plt.xlabel(\"sigma\")\n",
    "plt.ylabel(\"rmse\");\n",
    "\n",
    "plt.plot(sigmas, randomForest_rmse,'-o',label = 'random_forest')\n",
    "plt.plot(sigmas, decisionTree_rmse,'-o',label = 'decision_tree')\n",
    "plt.plot(sigmas, linear_rmse,'-o', label = 'linear_model')\n",
    "# plt.plot(width, constant_rmse,'-o',label = 'constant_model')\n",
    "# plt.suptitle('Example id = ' + str(idx))\n",
    "\n",
    "legend = plt.legend(loc='upper left', shadow=True)\n",
    "\n",
    "legend.get_frame()\n",
    "\n",
    "#plot the sigma v/s rmses \n",
    "# axes=plt.gca()\n",
    "\n",
    "# plt.xscale('log', basex = 2)\n",
    "# plt.xlabel(\"sigma\")\n",
    "# plt.ylabel(\"rmse\");\n",
    "\n",
    "# plt.plot(sigmas[0:4], randomForest_rmse[0:4],'-o',label = 'random_forest')\n",
    "# plt.plot(sigmas[0:4], decisionTree_rmse[0:4],'-o',label = 'decision_tree')\n",
    "# plt.plot(sigmas[0:4], linear_rmse[0:4],'-o', label = 'linear_model')\n",
    "# plt.plot(sigmas[0:4], constant_rmse[0:4],'-o',label = 'constant_model')\n",
    "# # plt.suptitle('Example id = ' + str(idx))\n",
    "\n",
    "# legend = plt.legend(loc='upper left', shadow=True)\n",
    "\n",
    "# legend.get_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #for plotting weight distribution of neighbourhood points for a particular sigma\n",
    "weights, predictions = tl.get_Weight_Distribution_Plot_Per_Sigma(25)\n",
    "sigma = 1\n",
    "#plot the weight distribution of neighbour points for a particular sigma\n",
    "axes=plt.gca()\n",
    "plt.ylabel(\"weights\")\n",
    "plt.xlabel(\"prediction\");\n",
    "plt.ylim(-0.5, 1.5)\n",
    "major_ticks = np.arange(-0.5, 1.5, 0.5)\n",
    "axes.set_yticks(major_ticks) \n",
    "plt.xlim(0,1)\n",
    "print(sigma)\n",
    "plt.plot(predictions, weights,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
